_target_: src.models.module.LitModule

# Number of classes (2 for binary case/control classification)
num_classes: 2

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0005
  weight_decay: 0.0001
  betas: [0.9, 0.999]

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.5
  patience: 5
  min_lr: 1.0e-6

net:
  _target_: src.models.components.transformer_cnn_net.TransformerCNNNet
  num_snps: 1000  # Number of SNPs to use for sequence creation
  encoding_dim: 1  # 1 for single value encoding (0, 1, 2)
  d_model: 64  # Transformer embedding dimension
  nhead: 8  # Number of attention heads
  num_transformer_layers: 2  # Number of transformer encoder layers
  cnn_channels: [64, 128, 32]  # CNN channel dimensions
  dropout: 0.25  # Dropout rate
  output_size: 2  # Number of output classes
  use_gradient_checkpointing: false  # Enable to save memory

# compile model for faster training with pytorch 2.0
compile: false
