# @package _global_

# to execute this experiment run:
# python train.py experiment=mental_stacked_lstm

defaults:
  - override /data: mental
  - override /model: stacked_lstm
  - override /callbacks: default
  - override /trainer: default
  - override /logger: wandb

tags: ["mental", "stacked_lstm", "case_control"]

seed: 42

trainer:
  min_epochs: 10
  max_epochs: 100
  gradient_clip_val: 1.0

callbacks:
  early_stopping:
    monitor: "val/loss"
    patience: 5
    mode: "min"
  
  model_checkpoint:
    monitor: "val/acc"
    mode: "max"
    save_top_k: 3

logger:
  wandb:
    name: "Mental-StackedLSTM-${model.net.hidden_sizes}-w${model.net.window_size}"
    tags: ${tags}
    group: "mental_classification"
