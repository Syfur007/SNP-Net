# @package _global_

# to execute this experiment run:
# python train.py experiment=dense_deep

defaults:
  - override /data: default
  - override /model: dense
  - override /callbacks: default
  - override /trainer: default
  - override /logger: wandb

# Deeper architecture variant for dense network

tags: ["dense", "deep", "case_control"]

seed: 12345

trainer:
  min_epochs: 10
  max_epochs: 100
  gradient_clip_val: 1.0
  
callbacks:
  early_stopping:
    monitor: "val/loss"
    patience: 20
    mode: "min"
  
  model_checkpoint:
    monitor: "val/acc"
    mode: "max"
    save_top_k: 3
    save_last: true

model:
  num_classes: 2
  optimizer:
    lr: 0.001
    weight_decay: 0.0001
  scheduler:
    mode: min
    factor: 0.5
    patience: 7
  net:
    hidden_sizes: [1024, 512, 256, 128, 64, 32]  # Deeper network
    output_size: 2
    dropout: 0.4  # Higher dropout for deeper network
    use_batch_norm: true
  compile: false

data:
  data_file: "data/snp_data.csv"
  labels_in_last_row: true
  batch_size: 64  # Larger batch size for stability
  train_val_test_split: [0.7, 0.15, 0.15]
  normalize: true
  num_workers: 4
  pin_memory: true
  # K-Fold Cross Validation (set to null to disable)
  num_folds: 5      # 5-fold cross validation
  current_fold: 0   # Will be overridden when running multiple folds

logger:
  wandb:
    name: "Dense-Deep-6Layer-1024-512-256-128-64-32-bs64"
    tags: ${tags}
    group: "classification"
  aim:
    experiment: "dense_deep"
