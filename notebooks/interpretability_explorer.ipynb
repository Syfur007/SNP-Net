{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ae23f1",
   "metadata": {},
   "source": [
    "# Interpretability Analysis Explorer\n",
    "\n",
    "Interactive exploration of SHAP, Integrated Gradients, and LIME results across multiple architectures and datasets.\n",
    "\n",
    "**Purpose**: Visualize and analyze SNP importance rankings for thesis/journal publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04296be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Setup plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a13b7",
   "metadata": {},
   "source": [
    "## 1. Load Interpretability Results\n",
    "\n",
    "Specify the base directory containing the analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure base directory for analysis results\n",
    "ANALYSIS_BASE_DIR = Path(project_root) / 'outputs' / 'interpretability_analysis'\n",
    "\n",
    "# Also check for alternative paths\n",
    "if not ANALYSIS_BASE_DIR.exists():\n",
    "    alt_paths = [\n",
    "        Path(project_root) / 'outputs',\n",
    "        Path('.') / 'outputs' / 'interpretability_analysis',\n",
    "    ]\n",
    "    \n",
    "    for alt_path in alt_paths:\n",
    "        if alt_path.exists():\n",
    "            if 'interpretability_analysis' not in str(alt_path):\n",
    "                ANALYSIS_BASE_DIR = alt_path / 'interpretability_analysis'\n",
    "            else:\n",
    "                ANALYSIS_BASE_DIR = alt_path\n",
    "            \n",
    "            if ANALYSIS_BASE_DIR.exists():\n",
    "                break\n",
    "\n",
    "print(f\"Analysis base directory: {ANALYSIS_BASE_DIR}\")\n",
    "print(f\"Exists: {ANALYSIS_BASE_DIR.exists()}\")\n",
    "\n",
    "# Discover available results\n",
    "available_results = {\n",
    "    'checkpoints': [],\n",
    "    'datasets': [],\n",
    "    'methods': ['shap', 'ig', 'lime']\n",
    "}\n",
    "\n",
    "if ANALYSIS_BASE_DIR.exists():\n",
    "    # Find checkpoint directories\n",
    "    for item in ANALYSIS_BASE_DIR.iterdir():\n",
    "        if item.is_dir() and item.name not in ['figures', 'data', 'publication_figures', 'supplementary_data']:\n",
    "            available_results['checkpoints'].append(item.name)\n",
    "            \n",
    "            # Find datasets within checkpoint\n",
    "            for dataset_item in item.iterdir():\n",
    "                if dataset_item.is_dir() and dataset_item.name not in ['shap', 'ig', 'lime']:\n",
    "                    if dataset_item.name not in available_results['datasets']:\n",
    "                        available_results['datasets'].append(dataset_item.name)\n",
    "\n",
    "print(f\"\\nüìä Available Results:\")\n",
    "print(f\"  Checkpoints: {len(available_results['checkpoints'])}\")\n",
    "print(f\"  Datasets: {len(available_results['datasets'])}\")\n",
    "print(f\"  Methods: {available_results['methods']}\")\n",
    "\n",
    "if not available_results['checkpoints']:\n",
    "    print(\"\\n‚ö†Ô∏è No analysis results found yet. Run these commands first:\")\n",
    "    print(\"  python src/interpretability_pipeline.py\")\n",
    "    print(\"  or\")\n",
    "    print(\"  python src/shap_explainability.py --checkpoint_path <path>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure base directory for analysis results\n",
    "# Update this path to your analysis output directory\n",
    "ANALYSIS_BASE_DIR = Path('outputs/interpretability_analysis')\n",
    "\n",
    "# Discover available results\n",
    "def discover_results(base_dir: Path) -> Dict[str, List[str]]:\n",
    "    \"\"\"Discover available checkpoints and datasets.\"\"\"\n",
    "    results = {\n",
    "        'checkpoints': [],\n",
    "        'datasets': [],\n",
    "        'methods': []\n",
    "    }\n",
    "    \n",
    "    if not base_dir.exists():\n",
    "        print(f\"‚ö†Ô∏è Analysis directory not found: {base_dir}\")\n",
    "        return results\n",
    "    \n",
    "    # Discover checkpoints\n",
    "    for checkpoint_dir in base_dir.iterdir():\n",
    "        if checkpoint_dir.is_dir() and checkpoint_dir.name != 'figures' and checkpoint_dir.name != 'data':\n",
    "            results['checkpoints'].append(checkpoint_dir.name)\n",
    "    \n",
    "    # Discover datasets and methods\n",
    "    if results['checkpoints']:\n",
    "        first_checkpoint = base_dir / results['checkpoints'][0]\n",
    "        for dataset_dir in first_checkpoint.iterdir():\n",
    "            if dataset_dir.is_dir():\n",
    "                results['datasets'].append(dataset_dir.name)\n",
    "                for method_dir in dataset_dir.iterdir():\n",
    "                    if method_dir.is_dir():\n",
    "                        results['methods'].append(method_dir.name)\n",
    "    \n",
    "    results['methods'] = list(set(results['methods']))\n",
    "    results['checkpoints'].sort()\n",
    "    results['datasets'].sort()\n",
    "    results['methods'].sort()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load results\n",
    "available_results = discover_results(ANALYSIS_BASE_DIR)\n",
    "print(f\"Discovered {len(available_results['checkpoints'])} checkpoints\")\n",
    "print(f\"Discovered {len(available_results['datasets'])} datasets\")\n",
    "print(f\"Discovered {len(available_results['methods'])} methods: {available_results['methods']}\")\n",
    "\n",
    "if available_results['checkpoints']:\n",
    "    print(f\"\\n‚úì Results loaded. Available checkpoints:\")\n",
    "    for ckpt in available_results['checkpoints'][:5]:\n",
    "        print(f\"  - {ckpt}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results found. Run interpretability_pipeline.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c09b0b",
   "metadata": {},
   "source": [
    "## 2. Utilities for SNP Analysis\n",
    "\n",
    "Load and compare SNP importance rankings across methods and architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_snp_ranking(checkpoint: str, dataset: str, method: str, base_dir: Path = ANALYSIS_BASE_DIR) -> pd.DataFrame:\n",
    "    \"\"\"Load SNP rankings for a specific analysis.\"\"\"\n",
    "    ranking_file = base_dir / checkpoint / dataset / method / f'top_{method}_snps.csv'\n",
    "    \n",
    "    if not ranking_file.exists():\n",
    "        print(f\"‚ö†Ô∏è File not found: {ranking_file}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.read_csv(ranking_file)\n",
    "\n",
    "def compare_top_snps_across_methods(checkpoint: str, dataset: str, top_k: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"Compare top-K SNPs across SHAP, IG, and LIME for a given checkpoint/dataset.\"\"\"\n",
    "    \n",
    "    rankings = {}\n",
    "    for method in available_results['methods']:\n",
    "        df = load_snp_ranking(checkpoint, dataset, method)\n",
    "        if not df.empty:\n",
    "            rankings[method] = df.head(top_k)[['Rank', 'SNP_ID']].set_index('SNP_ID')\n",
    "    \n",
    "    if not rankings:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine rankings\n",
    "    combined = pd.concat([rankings[m] for m in sorted(rankings.keys())], axis=1)\n",
    "    combined.columns = sorted(rankings.keys())\n",
    "    combined = combined.fillna('-')\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def get_top_consensus_snps(dataset: str, top_k: int = 30, min_agreement: float = 0.5) -> pd.DataFrame:\n",
    "    \"\"\"Get consensus SNPs across all architectures.\"\"\"\n",
    "    \n",
    "    snp_counts = {}\n",
    "    snp_ranks = {}\n",
    "    \n",
    "    for checkpoint in available_results['checkpoints']:\n",
    "        for method in available_results['methods']:\n",
    "            df = load_snp_ranking(checkpoint, dataset, method)\n",
    "            \n",
    "            if df.empty:\n",
    "                continue\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                snp_id = row['SNP_ID']\n",
    "                rank = row['Rank']\n",
    "                \n",
    "                if snp_id not in snp_counts:\n",
    "                    snp_counts[snp_id] = 0\n",
    "                    snp_ranks[snp_id] = []\n",
    "                \n",
    "                snp_counts[snp_id] += 1\n",
    "                snp_ranks[snp_id].append(rank)\n",
    "    \n",
    "    total_combinations = len(available_results['checkpoints']) * len(available_results['methods'])\n",
    "    \n",
    "    consensus_data = []\n",
    "    for snp_id, count in snp_counts.items():\n",
    "        agreement = count / total_combinations\n",
    "        \n",
    "        if agreement >= min_agreement:\n",
    "            consensus_data.append({\n",
    "                'SNP_ID': snp_id,\n",
    "                'Appearances': count,\n",
    "                'Agreement_Ratio': agreement,\n",
    "                'Mean_Rank': np.mean(snp_ranks[snp_id]),\n",
    "                'Std_Rank': np.std(snp_ranks[snp_id]),\n",
    "            })\n",
    "    \n",
    "    consensus_df = pd.DataFrame(consensus_data)\n",
    "    consensus_df = consensus_df.sort_values('Agreement_Ratio', ascending=False).head(top_k)\n",
    "    \n",
    "    return consensus_df\n",
    "\n",
    "print(\"‚úì Utility functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b2c38",
   "metadata": {},
   "source": [
    "## 3. Explore Top SNPs by Architecture\n",
    "\n",
    "Compare the top SNPs across different architectures and interpretability methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dfb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select checkpoint and dataset to explore\n",
    "if available_results['checkpoints']:\n",
    "    selected_checkpoint = available_results['checkpoints'][0]  # First checkpoint by default\n",
    "    selected_dataset = available_results['datasets'][0] if available_results['datasets'] else 'autism'\n",
    "    \n",
    "    print(f\"Currently viewing: {selected_checkpoint} / {selected_dataset}\")\n",
    "    print(f\"\\nTo explore different results, modify selected_checkpoint or selected_dataset\")\n",
    "    print(f\"\\nAvailable checkpoints: {len(available_results['checkpoints'])}\")\n",
    "    print(f\"Available datasets: {len(available_results['datasets'])}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results available to explore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7fb0c",
   "metadata": {},
   "source": [
    "## 4. Compare Top SNPs Across Methods\n",
    "\n",
    "View how the top-20 SNPs compare across SHAP, IG, and LIME for a given architecture/dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f9a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare top SNPs across methods\n",
    "if available_results['checkpoints']:\n",
    "    comparison_df = compare_top_snps_across_methods(selected_checkpoint, selected_dataset, top_k=20)\n",
    "    \n",
    "    if not comparison_df.empty:\n",
    "        print(f\"Top 20 SNPs Comparison ({selected_checkpoint} - {selected_dataset}):\\n\")\n",
    "        print(comparison_df.to_string())\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No data available for this checkpoint/dataset combination\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aec6bc",
   "metadata": {},
   "source": [
    "## 5. Consensus SNPs Across Architectures\n",
    "\n",
    "Identify SNPs that are consistently important across multiple architectures and methods (robust for publication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52103be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute consensus SNPs for autism dataset\n",
    "if available_results['datasets']:\n",
    "    consensus_snps = get_top_consensus_snps(selected_dataset, top_k=30, min_agreement=0.3)\n",
    "    \n",
    "    if not consensus_snps.empty:\n",
    "        print(f\"\\nConsensus SNPs for {selected_dataset}:\")\n",
    "        print(f\"(SNPs appearing in ‚â•30% of architecture/method combinations)\\n\")\n",
    "        print(consensus_snps.to_string(index=False))\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\nüìä Summary:\")\n",
    "        print(f\"  Total consensus SNPs: {len(consensus_snps)}\")\n",
    "        print(f\"  Mean agreement ratio: {consensus_snps['Agreement_Ratio'].mean():.2%}\")\n",
    "        print(f\"  Max agreement ratio: {consensus_snps['Agreement_Ratio'].max():.2%}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No consensus SNPs found with current threshold\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No datasets available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5c753",
   "metadata": {},
   "source": [
    "## 6. Generate Publication-Quality Figures\n",
    "\n",
    "Create high-resolution figures suitable for thesis and journal articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for figures\n",
    "FIGURE_OUTPUT_DIR = Path(ANALYSIS_BASE_DIR) / 'publication_figures'\n",
    "FIGURE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Publication figures will be saved to: {FIGURE_OUTPUT_DIR}\")\n",
    "\n",
    "# Figure 1: Consensus SNPs bar plot\n",
    "def create_consensus_figure(dataset: str, top_k: int = 30):\n",
    "    \"\"\"Create consensus SNP visualization.\"\"\"\n",
    "    consensus_df = get_top_consensus_snps(dataset, top_k=top_k, min_agreement=0.2)\n",
    "    \n",
    "    if consensus_df.empty:\n",
    "        print(f\"No data for {dataset}\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    colors = plt.cm.viridis(consensus_df['Agreement_Ratio'] / consensus_df['Agreement_Ratio'].max())\n",
    "    bars = ax.barh(range(len(consensus_df)), consensus_df['Agreement_Ratio'], color=colors)\n",
    "    \n",
    "    ax.set_yticks(range(len(consensus_df)))\n",
    "    ax.set_yticklabels(consensus_df['SNP_ID'], fontsize=9)\n",
    "    ax.set_xlabel('Consensus Ratio (fraction of models)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('SNP Identifier', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Top {top_k} Consensus SNPs\\n{dataset.capitalize()} Dataset - Robust Across Multiple Architectures', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, consensus_df['Agreement_Ratio'])):\n",
    "        ax.text(val + 0.01, i, f'{val:.0%}', va='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_file = FIGURE_OUTPUT_DIR / f'consensus_snps_{dataset}_top{top_k}.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved: {output_file}\")\n",
    "    plt.close()\n",
    "\n",
    "# Generate figures for each dataset\n",
    "for dataset in available_results['datasets']:\n",
    "    create_consensus_figure(dataset, top_k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a3bdc0",
   "metadata": {},
   "source": [
    "## 7. Export Results for Publication\n",
    "\n",
    "Save consensus SNP rankings and summary statistics for supplementary materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all consensus SNPs to CSV for supplementary materials\n",
    "DATA_OUTPUT_DIR = Path(ANALYSIS_BASE_DIR) / 'supplementary_data'\n",
    "DATA_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for dataset in available_results['datasets']:\n",
    "    consensus_df = get_top_consensus_snps(dataset, top_k=200, min_agreement=0.2)\n",
    "    \n",
    "    if not consensus_df.empty:\n",
    "        output_file = DATA_OUTPUT_DIR / f'consensus_snps_{dataset}_full.csv'\n",
    "        consensus_df.to_csv(output_file, index=False)\n",
    "        print(f\"‚úì Exported: {output_file} ({len(consensus_df)} SNPs)\")\n",
    "\n",
    "print(f\"\\n‚úì All results exported to: {DATA_OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
